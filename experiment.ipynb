{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CustomerId    Surname  CreditScore Geography  Gender  Age  Tenure  \\\n",
       "0       15634602   Hargrave          619    France  Female   42       2   \n",
       "1       15647311       Hill          608     Spain  Female   41       1   \n",
       "2       15619304       Onio          502    France  Female   42       8   \n",
       "3       15701354       Boni          699    France  Female   39       1   \n",
       "4       15737888   Mitchell          850     Spain  Female   43       2   \n",
       "...          ...        ...          ...       ...     ...  ...     ...   \n",
       "9995    15606229   Obijiaku          771    France    Male   39       5   \n",
       "9996    15569892  Johnstone          516    France    Male   35      10   \n",
       "9997    15584532        Liu          709    France  Female   36       7   \n",
       "9998    15682355  Sabbatini          772   Germany    Male   42       3   \n",
       "9999    15628319     Walker          792    France  Female   28       4   \n",
       "\n",
       "        Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
       "0          0.00              1          1               1        101348.88   \n",
       "1      83807.86              1          0               1        112542.58   \n",
       "2     159660.80              3          1               0        113931.57   \n",
       "3          0.00              2          0               0         93826.63   \n",
       "4     125510.82              1          1               1         79084.10   \n",
       "...         ...            ...        ...             ...              ...   \n",
       "9995       0.00              2          1               0         96270.64   \n",
       "9996   57369.61              1          1               1        101699.77   \n",
       "9997       0.00              1          0               1         42085.58   \n",
       "9998   75075.31              2          1               0         92888.52   \n",
       "9999  130142.79              1          1               0         38190.78   \n",
       "\n",
       "      Exited  \n",
       "0          1  \n",
       "1          0  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "9995       0  \n",
       "9996       0  \n",
       "9997       1  \n",
       "9998       1  \n",
       "9999       0  \n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('RowNumber', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(df.sample(frac=1), [int(0.8*len(df)), int(0.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 training examples\n",
      "1000 validation examples\n",
      "1000 test examples\n"
     ]
    }
   ],
   "source": [
    "print(len(train), 'training examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(df, shuffle=True, batch_size=32):\n",
    "  df = df.copy()\n",
    "  labels = df.pop('Exited')\n",
    "  df = {key: value[:,tf.newaxis] for key, value in df.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(df))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OlumayowaOyaleke\\AppData\\Local\\Temp\\ipykernel_15152\\1009109474.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in df.items()}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
      "A batch of ages: tf.Tensor(\n",
      "[[b'France']\n",
      " [b'France']\n",
      " [b'France']\n",
      " [b'France']\n",
      " [b'France']], shape=(5, 1), dtype=string)\n",
      "A batch of targets: tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of ages:', train_features['Geography'])\n",
    "print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[ 0.34424615],\n",
       "       [ 1.3288893 ],\n",
       "       [-0.7025849 ],\n",
       "       [-1.0549835 ],\n",
       "       [-1.0031602 ]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_count_col = train_features['CreditScore']\n",
    "layer = get_normalization_layer('CreditScore', train_ds)\n",
    "layer(photo_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_type_col = train_features['Geography']\n",
    "test_type_layer = get_category_encoding_layer(name='Geography',\n",
    "                                              dataset=train_ds,\n",
    "                                              dtype='string')\n",
    "test_type_layer(test_type_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_type_col = train_features['Gender']\n",
    "test_type_layer = get_category_encoding_layer(name='Gender',\n",
    "                                              dataset=train_ds,\n",
    "                                              dtype='string')\n",
    "test_type_layer(test_type_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OlumayowaOyaleke\\AppData\\Local\\Temp\\ipykernel_15152\\1009109474.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in df.items()}\n",
      "C:\\Users\\OlumayowaOyaleke\\AppData\\Local\\Temp\\ipykernel_15152\\1009109474.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in df.items()}\n",
      "C:\\Users\\OlumayowaOyaleke\\AppData\\Local\\Temp\\ipykernel_15152\\1009109474.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in df.items()}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
       "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
       "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RowNumber',\n",
       " 'CustomerId',\n",
       " 'Surname',\n",
       " 'CreditScore',\n",
       " 'Geography',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Tenure',\n",
       " 'Balance',\n",
       " 'NumOfProducts',\n",
       " 'HasCrCard',\n",
       " 'IsActiveMember',\n",
       " 'EstimatedSalary',\n",
       " 'Exited']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
    "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "       'IsActiveMember', 'EstimatedSalary', 'Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'Age','Balance','CreditScore','CustomerId','EstimatedSalary','HasCrCard','IsActiveMember','NumOfProducts','RowNumber','Tenure'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FEATURES = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numerical features.\n",
    "for header in SCALE_FEATURES:\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_KEYS = ['Geography','Gender']\n",
    "\n",
    "for header in CATEGORICAL_FEATURE_KEYS:\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                               dataset=train_ds,\n",
    "                                               dtype='string',\n",
    "                                               max_tokens=5)\n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 13) dtype=float32 (created by layer 'concatenate_2')>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.ndim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Reshape((12,), input_shape = (1, 12))(inputs)\n",
    "\n",
    "x = tf.keras.layers.Dense(11, activation='relu')(x) \n",
    "x = tf.keras.layers.Dense(11, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(11, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# Use `rankdir='LR'` to make the graph horizontal.\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OlumayowaOyaleke\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\functional.py:566: UserWarning: Input dict contained keys ['RowNumber', 'CustomerId', 'Surname', 'HasCrCard', 'IsActiveMember'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 15ms/step - loss: 0.8123 - accuracy: 0.6220 - val_loss: 0.6721 - val_accuracy: 0.7110\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6360 - accuracy: 0.7481 - val_loss: 0.5648 - val_accuracy: 0.7800\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.5578 - accuracy: 0.7912 - val_loss: 0.5162 - val_accuracy: 0.7810\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.5187 - accuracy: 0.7956 - val_loss: 0.4955 - val_accuracy: 0.7810\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4971 - accuracy: 0.7993 - val_loss: 0.4846 - val_accuracy: 0.7790\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.7985 - val_loss: 0.4776 - val_accuracy: 0.7790\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.4745 - accuracy: 0.7997 - val_loss: 0.4724 - val_accuracy: 0.7790\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4607 - accuracy: 0.8019 - val_loss: 0.4687 - val_accuracy: 0.7800\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4615 - accuracy: 0.8014 - val_loss: 0.4651 - val_accuracy: 0.7780\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4562 - accuracy: 0.8016 - val_loss: 0.4616 - val_accuracy: 0.7810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f4b7154a30>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4375 - accuracy: 0.7890\n",
      "Accuracy 0.7889999747276306\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define imports\n",
    "from pyexpat import model\n",
    "# from kerastuner.engine import base_tuner\n",
    "# import kerastuner as kt\n",
    "from tensorflow import keras\n",
    "from typing import NamedTuple, Dict, Text, Any\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs  \n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "import os\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from tfx.dsl.io import fileio\n",
    "from tfx.utils import io_utils\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import numpy as np\n",
    "import absl\n",
    "# import tensorflow_decision_forests as tfdf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "ONE_HOT_FEATURES2   = {\n",
    "    'Gender':2,'Geography':3\n",
    "}\n",
    "\n",
    "ONE_HOT_FEATURES   = [\n",
    "    'Gender','Geography'\n",
    "]\n",
    "\n",
    "BUCKETIZE = {\n",
    "    'Age' : 10\n",
    "}\n",
    "\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'Age','Balance','CreditScore','CustomerId','EstimatedSalary','HasCrCard','IsActiveMember','NumOfProducts','RowNumber','Tenure'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = ['Geography','Gender']\n",
    "\n",
    "SCALE_FEATURES = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']\n",
    "\n",
    "LABEL_KEY = 'Exited'\n",
    "\n",
    "# Renamimg Features   \n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "#Define Callbacks\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 10)\n",
    "\n",
    "#Load compressed data\n",
    "def _gzip_reader_fn(filenames):\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type= 'GZIP')\n",
    "\n",
    "#Load data\n",
    "def _input_fn(file_pattern: str, tf_transform_output: tft.TFTransformOutput, num_epochs= None, batch_size: int = 200,) -> tf.data.Dataset:\n",
    "\n",
    "    # Get post transform feature specification\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    #create batches of features and labels\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern= file_pattern,\n",
    "        batch_size= batch_size,\n",
    "        features= transformed_feature_spec,\n",
    "        reader = _gzip_reader_fn,\n",
    "        num_epochs= num_epochs,\n",
    "        label_key= transformed_name(LABEL_KEY)\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "  \n",
    "\n",
    "#Build model\n",
    "def get_model():\n",
    "    #One-hot Categorical Features\n",
    "    input_features = []\n",
    "    for key in ONE_HOT_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape = (1,),\n",
    "            dtype = \"string\",\n",
    "            name = transformed_name(key))\n",
    "        )\n",
    "    #Scale Features\n",
    "    for key in SCALE_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape = (1,),\n",
    "            name = transformed_name(key))\n",
    "        )\n",
    "    \n",
    "    inputs = input_features\n",
    "    all_inputs = tf.keras.layers.concatenate(inputs)\n",
    "    #reshaped_narrative = tf.reshape(inputs[0], [-1])\n",
    "\n",
    "    #x = tf.keras.layers.Reshape((3, 4), input_shape = (12,))(all_inputs)\n",
    "\n",
    "    # d = tf.keras.layers.concatenate(inputs)\n",
    "      \n",
    "    x = tf.keras.layers.Dense(8, activation='relu')(all_inputs) \n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(16, activation='sigmoid')(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(3, activation = 'sigmoid')(x)\n",
    "\n",
    "    keras_model = tf.keras.Model(inputs= inputs, outputs=outputs) \n",
    "\n",
    "    keras_model.compile(   \n",
    "                   optimizer=tf.keras.optimizers.Adam(1e-2), \n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
    "                    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "                             tf.keras.metrics.BinaryAccuracy(),\n",
    "                             tf.keras.metrics.TruePositives()])\n",
    "    keras_model.summary()\n",
    "    return keras_model\n",
    " \n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        \n",
    "        feature_spec.pop(\"Exited\")\n",
    "        \n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        \n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        \n",
    "        # get predictions using the transformed features\n",
    "        return model(transformed_features)\n",
    "        \n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "#Run\n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir = fn_args.model_run_dir, update_freq = 'batch'\n",
    "    )\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max', verbose = 1, patience = 10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor = 'val_binary_accuracy', mode = 'max', verbose =1, save_best_only = True)\n",
    "\n",
    "    #\n",
    "    schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema())\n",
    "\n",
    "    # Load tf_transform_output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Create batches of data\n",
    "    train_set = _input_fn(fn_args.train_files, tf_transform_output, 10)\n",
    "    eval_set = _input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files,tf_transform_output)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files,tf_transform_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "def get_model():\n",
    "    #One-hot Categorical Features\n",
    "    input_features = []\n",
    "    for key in ONE_HOT_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape = (1,),\n",
    "            dtype = \"string\",\n",
    "            name = transformed_name(key))\n",
    "        )\n",
    "    #Scale Features\n",
    "    for key in SCALE_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape = (1,),\n",
    "            name = transformed_name(key))\n",
    "        )\n",
    "    \n",
    "    inputs = input_features\n",
    "    all_inputs = tf.keras.layers.concatenate(inputs)\n",
    "    #reshaped_narrative = tf.reshape(inputs[0], [-1])\n",
    "\n",
    "    #x = tf.keras.layers.Reshape((3, 4), input_shape = (12,))(all_inputs)\n",
    "\n",
    "    # d = tf.keras.layers.concatenate(inputs)\n",
    "      \n",
    "    x = tf.keras.layers.Dense(8, activation='relu')(all_inputs) \n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(16, activation='sigmoid')(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(3, activation = 'sigmoid')(x)\n",
    "\n",
    "    keras_model = tf.keras.Model(inputs= inputs, outputs=outputs) \n",
    "\n",
    "    keras_model.compile(   \n",
    "                   optimizer=tf.keras.optimizers.Adam(1e-2), \n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
    "                    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "                             tf.keras.metrics.BinaryAccuracy(),\n",
    "                             tf.keras.metrics.TruePositives()])\n",
    "    keras_model.summary()\n",
    "    return keras_model\n",
    " \n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        \n",
    "        feature_spec.pop(\"Exited\")\n",
    "        \n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        \n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        \n",
    "        # get predictions using the transformed features\n",
    "        return model(transformed_features)\n",
    "        \n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "#Run\n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir = fn_args.model_run_dir, update_freq = 'batch'\n",
    "    )\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max', verbose = 1, patience = 10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor = 'val_binary_accuracy', mode = 'max', verbose =1, save_best_only = True)\n",
    "\n",
    "    #\n",
    "    schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema())\n",
    "\n",
    "    # Load tf_transform_output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Create batches of data\n",
    "    train_set = _input_fn(fn_args.train_files, tf_transform_output, 10)\n",
    "    eval_set = _input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files,tf_transform_output)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files,tf_transform_output)\n",
    "\n",
    "    # Build the model\n",
    "    model = get_model()\n",
    "    model.fit(##tf.expand_dims(train_set, axis= -1)##,\n",
    "              train_set, \n",
    "              validation_steps = 32, \n",
    "              validation_data = eval_set)\n",
    "    absl.logging.info(model)\n",
    "\n",
    "    evaluation = model.evaluate(eval_set, steps = 32)\n",
    "    absl.logging.info('Accuracy: %f', evaluation)\n",
    "    \n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "        _get_serve_tf_examples_fn(model, \n",
    "                                 tf_transform_output).get_concrete_function(\n",
    "                                    tf.TensorSpec(\n",
    "                                    shape=[None],\n",
    "                                    dtype=tf.string,\n",
    "                                    name='examples')) \n",
    "    }\n",
    "    # model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
    "    model.save(fn_args.serving_model_dir,\n",
    "               signatures=signatures, \n",
    "               save_format='tf')\n",
    "\n",
    "    # Export the model as a pickle named model.pkl. AI Platform Prediction expects\n",
    "  # sklearn model artifacts to follow this naming convention.\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'FnArgs' has no attribute 'schema_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15152\\916306684.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_pbtxt_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Load tf_transform_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'FnArgs' has no attribute 'schema_file'"
     ]
    }
   ],
   "source": [
    "fn_args = FnArgs\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "#       log_dir = fn_args.model_run_dir, update_freq = 'batch'\n",
    "    # )\n",
    "# es = tf.keras.callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max', verbose = 1, patience = 10)\n",
    "# mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor = 'val_binary_accuracy', mode = 'max', verbose =1, save_best_only = True)\n",
    "\n",
    "    #\n",
    "schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema())\n",
    "\n",
    "    # Load tf_transform_output\n",
    "tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Create batches of data\n",
    "train_set = _input_fn(fn_args.train_files, tf_transform_output, 10)\n",
    "eval_set = _input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
    "\n",
    "train_dataset = _input_fn(fn_args.train_files,tf_transform_output)\n",
    "eval_dataset = _input_fn(fn_args.eval_files,tf_transform_output)\n",
    "\n",
    "    # Build the model\n",
    "model = get_model()\n",
    "model.fit(##tf.expand_dims(train_set, axis= -1)##,\n",
    "              train_set, \n",
    "              validation_steps = 32, \n",
    "              validation_data = eval_set)\n",
    "absl.logging.info(model)\n",
    "\n",
    "evaluation = model.evaluate(eval_set, steps = 32)\n",
    "absl.logging.info('Accuracy: %f', evaluation)\n",
    "    \n",
    "signatures = {\n",
    "        'serving_default':\n",
    "        _get_serve_tf_examples_fn(model, \n",
    "                                 tf_transform_output).get_concrete_function(\n",
    "                                    tf.TensorSpec(\n",
    "                                    shape=[None],\n",
    "                                    dtype=tf.string,\n",
    "                                    name='examples')) \n",
    "    }\n",
    "    # model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
    "model.save(fn_args.serving_model_dir,\n",
    "               signatures=signatures, \n",
    "               save_format='tf')\n",
    "\n",
    "    # Export the model as a pickle named model.pkl. AI Platform Prediction expects\n",
    "  # sklearn model artifacts to follow this naming convention.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "208155b8403608ff43652ec57568cfee279cf1c9a3f01b1d9d255e044896b23a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
